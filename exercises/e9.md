# Exercise Set IX

### Exercise 72

Consider functions $f_\epsilon(x) = (1 + \epsilon)^x$ and $g_\epsilon(x) = 1 + \epsilon x$ with $x \in \[0; 1\]$ and $\epsilon \in \[-\frac{1}{2}; \frac{1}{2}\] \setminus \\{0\\}$.
We have $f_\epsilon(0) = g_\epsilon(0) = 1$ and $f_\epsilon(1) = g_\epsilon(1) = 1 + \epsilon$.
Note that $g_\epsilon$ is a line connecting the endpoints of $f_\epsilon$ on the interval $\[0; 1\]$.
Next, $f_\epsilon^\prime(x) = (1 + \epsilon)^x \log(1 + \epsilon)$ and $f_\epsilon^{\prime \prime}(x) = (1 + \epsilon)^x \log^2(1 + \epsilon)$.
The latter is positive for all possible $x$ and $\epsilon$, so $f_\epsilon$ is convex.
Then, by definition of convexity, $f_\epsilon$, evaluated at any point within an interval $\[0; 1\]$, is below the value of a linear function $g_\epsilon$ evaluated at the same point, which gives us the desired result.

### Exercise 73

Start with $i = 0$.
On every iteration, increase $i$ by $1$ and run multiplicative weights from scratch for (at most) $T_i = 2^i$ timestamps with $\epsilon_i = \sqrt{\frac{\ln n}{T_i}}$.
If MW doesn't terminate within $T_i$ timestamps, proceed to the next iteration.
Otherwise, return the solution, which is guaranteed to have the expected cost at most $\Theta\left(\sqrt{\frac{\ln n}{T}}\right)$ more than $\mathrm{OPT}$.

### Exercise 74

Compare $\Gamma^T$ and $\mathrm{OPT}$ (recall that $c^t(a) \in \[0; 1\]$):

$$
\Gamma^T \ge w^T(a^\ast)
    = \underbrace{w^1(a^\ast)}\_{=1} \prod_{t=1}^T
        \underbrace{(1 - \epsilon c^t(a^\ast))}\_{\ge (1-\epsilon)^{c^t(a^\ast)}}
    \ge (1 - \epsilon)^{\mathrm{OPT}}.
$$

Relate $\Gamma^t$ and $\Gamma^{t+1}$:

$$
\Gamma^{t+1}
    = \sum_{a \in \mathcal A} w^{(t+1)}(a)
    = \sum_{a \in \mathcal A} w^t(a) (1 - \epsilon c^t(a))
    = \Gamma^t (1 - \epsilon \nu^t) \\
\Rightarrow
\Gamma^T = \underbrace{\Gamma^1}\_{= n} \prod_{t=1}^T (1 - \epsilon \nu^t).
$$

So, we again arrive at

$$
(1 - \epsilon)^{\mathrm{OPT}} \le n \prod_{t=1}^T (1 - \epsilon \nu^t).
$$

Rest of analysis is the same.

### Exercise 75

Let $a^\*$ be the payoff-maximizing action and $\mathrm{OPT} = \sum\limits_{t=1}^T \pi^t(a^\ast)$.

Compare $\Gamma^T$ and $OPT$:

$$
\Gamma^T \ge w^T(a^\ast)
    = \underbrace{w^1(a^\ast)}\_{=1} \prod_{t=1}^T (1 + \epsilon)^{\pi^t(a^\ast)}
    \ge (1 + \epsilon)^{\mathrm{OPT}}. \ \ \ \ (1)
$$

Relate $\Gamma^t$ and $\Gamma^{t+1}:$

$$
\Gamma^{t+1}
    = \sum_{a \in \mathcal A} w^{(t+1)}(a)
    = \sum_{a \in \mathcal A} w^t(a) \underbrace{(1 + \epsilon)^{\pi^t(a)}}\_{\le 1 + \epsilon \pi^t(a)}
    \le \Gamma^t (1 + \epsilon \nu^t),
$$

where $\nu^t = \sum_{a \in \mathcal A} \frac{w^t(a)}{\Gamma^t} \pi^t(a)$ is expected payoff of the algorithm at iteration $t$. 
Then,

$$
\Gamma^T \le \underbrace{\Gamma^1}\_{= n} \prod_{t=1}^T (1 + \epsilon \nu^t). \ \ \ \ (2)
$$

Combine $(1)$ and $(2)$, take logs:

$$
\mathrm{OPT}\ \ln(1 + \epsilon) \le \ln n + \sum_{t=1}^T \ln{(1 + \epsilon \nu^t)}. \ \ \ \ (3)
$$

Use Taylor series expansion

$$
\ln(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \dots
$$

to get that for $\epsilon \in \[0; 1/2)$ (remember that $\nu^t \le 1$)

$$
\ln(1 + \epsilon \nu^t) \le \epsilon \nu^t
$$

and

$$
\ln(1 + \epsilon) \ge \epsilon - \epsilon^2.
$$

Now we can strengthen $(3)$ and write

$$
\sum_{t=1}^T \nu^t 
    \ge \mathrm{OPT} - \epsilon \mathrm{OPT} - \frac{\ln n}{\epsilon} 
    \ge \mathrm{OPT} - \epsilon T - \frac{\ln n}{\epsilon}.
$$

Setting $\epsilon$ to $\sqrt \frac{\ln n}{T}$ gives us that the time-averaged expected regret of the algorithm is at most $2 \sqrt \frac{\ln n}{T}$.

### Exercise 76

If costs are in the range $\[0; c_{\max}\]$, we can use the following weights update formula:

$$
w^{t+1}(a) = w^t(a) (1 - \epsilon)^{c^t(a) / c_{\max}}.
$$

This way, we still can apply all the analysis.

Relation between $\Gamma^T$ and $\mathrm{OPT}$:

$$
\Gamma^T \ge (1 - \epsilon)^{\mathrm{OPT} / c_{\max}}.
$$

Relation between $\Gamma^T$ and expected cost:

$$
\Gamma^T = n \prod_{t=1}^T(1 - \epsilon \nu^t).
$$

Now $\nu^t = \frac{1}{c_{\max}} \sum_{a \in \mathcal A} \frac{w^t(a)}{\Gamma^t} c^t(a)$ is expected cost of the algorithm at iteration $t$, divided by the maximum possible cost.

Continuing the analysis as before, we arrive at

$$
\sum_{t=1}^T \nu^t \le \frac{\mathrm{OPT}}{c_{\max}} + T \sqrt \frac{\ln n}{T}. 
$$

Multiplying both sides by $c_{\max}$, subtracting $\mathrm{OPT}$ from both sides, and dividing by $T$, we get that the time-averaged expected regret of the algorithm is at most $2 c_{\max} \sqrt \frac{\ln n}{T}$.

### Exercise 77

Let $\mathbf s^\ast$ be an optimal outcome. We know that $\sigma$ is such that for a given player $i$ and every $t$ the expected time-averaged regret is $R_i$, which implies

$$
\frac{1}{T} \left\[
    \mathbb E_{\mathbf s \sim \sigma^t} \[c_i(\mathbf{s})\]
    - \mathbb E_{\mathbf s \sim \sigma^t} \[c_i(s_i^\ast, \mathbf s_{-i})\]
\right\]
\le R_i,
$$

or

$$
\frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \[c_i(\mathbf{s})\]
\le
\frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \[c_i(s_i^\ast, \mathbf s_{-i})\] + R_i.
\ \ \ \ (\ast)
$$

Since the game is $(\lambda, \mu)$-smooth, we also know that

$$
\frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \left\[ \sum_{i=1}^k c_i(s_i^\ast, \mathbf s_{-i}) \right\]
\le
\lambda \mathrm{cost}(\mathbf s^\ast) + \mu \frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \[\mathrm{cost}(\mathbf s)\].
\ \ \ \ (\ast\ast)
$$

Next,

$$
\displaylines{
\frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \[\mathrm{cost}(\mathbf s)\]
    \underset{\text{linearity of expectation}}{=} \sum_{i=1}^k
        \frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \[c_i(\mathbf{s})\] \\
    \underset{(\ast)}{\le}
        \frac{1}{T} \sum_{i=1}^k \mathbb E_{\mathbf s \sim \sigma^t} \[c_i(s_i^\ast, \mathbf s_{-i})\]
        + \sum_{i=1}^k R_i \\
    \underset{(\ast\ast)}{\le}
        \lambda \mathrm{cost}(\mathbf s^\ast)
        + \mu \frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \[\mathrm{cost}(\mathbf s)\]
        + \sum_{i=1}^k R_i.
}
$$

After simple manipulations, we get the desired result:

$$
\mathbb E_{\mathbf s \sim \sigma} \[\mathrm{cost}(\mathbf s)\]
    = \frac{1}{T} \mathbb E_{\mathbf s \sim \sigma^t} \[\mathrm{cost}(\mathbf s)\]
    \le \frac{\lambda}{1 - \mu} \mathrm{cost}(\mathbf s^\ast) + \frac{1}{1 - \mu} \sum_{i=1}^k R_i.
$$

### Exercise 78

:(

### Exercise 79

The time-averaged expected external regret of a $j$-th multiplicative weights algorithm is given by

$$
R_j = 2 \sqrt \frac{\ln n}{T}.
$$

The time-averaged swap-regret of the master algorithm equals the sum of external regrets of $n$ child MV algorithms, which is

$$
2n \sqrt \frac{\ln n}{T}.
$$

### Exercise 80

Let's assume that $\hat x$ and $\hat y$ form a Mixed Nash Equilibrium.

First, $\hat x$ is a best response to $\hat y$, so

$$
\forall x,\ \hat x^\top A \hat y \ge x^\top A \hat y \ge \min_y x^\top A y,
$$

which implies

$$
\hat x^\top A \hat y \ge \max_x \min_y x^\top A y. \ \ \ \ (1)
$$

Similarly, $\hat y$ is a best response to $\hat x$, so

$$
\forall y,\ \hat x^\top A \hat y \le \hat x^\top A y \le \max_x x^\top A y,
$$

meaning

$$
\hat x^\top A \hat y \le \min_y \max_x x^\top A y. \ \ \ \ (2)
$$

From the lecture we know that right-hand sides of inequalities $(1)$ and $(2)$ are equal, so

$$
\hat x^\top A \hat y = \max_x \min_y x^\top A y = \min_y \max_x x^\top A y,
$$

which means that $\hat x$ and $\hat y$ also form a minimax pair.

The reverse implication simply follows from the fact that NRD strategies that were constructed to converge to a minimax pair also converge to a MNE.
Let $\hat x$ and $\hat y$ be the time-averaged mixed strategies of NRD after $T$ iterations.
In the lecture we've shown that

$$
\displaylines{
\forall x,\ x^\top A \hat y \le v + \epsilon, \\
\forall y,\ \hat x^\top A y \ge v - \epsilon,
}
$$

where $v$ is a minimax value of the game and $\epsilon$ is time-averaged regret.

As $T \to \infty$, the regret $\epsilon$ goes to $0$, $\hat x^\top A \hat y$ converges to $v$, and the inequalities turn into

$$
\displaylines{
\forall x,\ x^\top A \hat y \le \hat x^\top A \hat y, \\
\forall y,\ \hat x^\top A y \ge \hat x^\top A \hat y,
}
$$

which is exactly the definition of MNE.

### Exercise 81

Let $A \in \mathbb R^{m \times n}$ be the payoff matrix of the game.

Since $(x_1, y_1)$ and $(x_2, y_2)$ are Nash Equilibria, we can write

$$
\begin{aligned}
\forall x \in \mathbb R^m,\ & x_1^\top A y_1 \ge x^\top A y_1, &\ \ \ \ (1a) \\
\forall y \in \mathbb R^n,\ & x_1^\top A y_1 \le x_1^\top A y, &\ \ \ \ (1b) \\
\forall x \in \mathbb R^m,\ & x_2^\top A y_2 \ge x^\top A y_2, &\ \ \ \ (2a) \\
\forall y \in \mathbb R^n,\ & x_2^\top A y_2 \le x_2^\top A y. &\ \ \ \ (2b)
\end{aligned}
$$

Let's show that $(x_1, y_2)$ is a Nash Equilibria.
Fix arbitrary $x \in \mathbb R^m$ and $y \in \mathbb R^n$.
We can write down two nice equation chains:

$$
x_1^\top A y_2
    \underset{(1b)}{\ge} x_1^\top A y_1
    \underset{(1a)}{\ge} x_2^\top A y_1
    \underset{(2b)}{\ge} x_2^\top A y_2
    \underset{(2a)}{\ge} x^\top A y_2,
$$

$$
x_1^\top A y_2
    \underset{(2a)}{\le} x_2^\top A y_2
    \underset{(2b)}{\le} x_2^\top A y_1
    \underset{(1a)}{\le} x_1^\top A y_1
    \underset{(1b)}{\le} x_1^\top A y.
$$

The first of them means that $x_1$ is a best response to $y_2$, and the second &mdash; that $y_2$ is a best response to $x_1$. Together this means that $(x_1, y_2)$ is a Nash Equilibria. We can easily repeat the argument to show that $(x_2, y_1)$ is also a NE.
